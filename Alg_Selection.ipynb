{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2482495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author: Michael Elgin (melgin@uwyo.edu)\n",
    "#2023_10_11\n",
    "\n",
    "#Notebook for ML Algorithm Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93bc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules\n",
    "\n",
    "#Python 3.11.2\n",
    "import numpy as np #1.26.0\n",
    "import pandas as pd #1.5.3\n",
    "from sklearn.model_selection import train_test_split #1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08775bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 - Algorithm selection for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1c3bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N for red wine: 1599\n",
      "N for white wine: 4898\n"
     ]
    }
   ],
   "source": [
    "#Wine color shall be chosen based on which dataset has larger N\n",
    "\n",
    "df_red = pd.read_csv(\"data/winequality-red.csv\", sep=\";\")\n",
    "df_white = pd.read_csv(\"data/winequality-white.csv\", sep=\";\")\n",
    "\n",
    "print(\"N for red wine: {0}\".format(len(df_red)))\n",
    "print(\"N for white wine: {0}\".format(len(df_white)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "297e3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All algorithms up for selection will now be evaluated on the white wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49cfb819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next the white wine data will be split into training and test sets\n",
    "#For fairness in algorithm selection, these will be the same sets used to train and test all models\n",
    "all_white_wine = df_white.to_numpy()\n",
    "train, test = train_test_split(all_white_wine, test_size=0.2, random_state=0) #Remember - also shuffles\n",
    "X_train = train[:, 0:-1] #Features\n",
    "y_train = train[:, -1] #Target\n",
    "X_test = test[:, 0:-1] #Features\n",
    "y_test = test[:, -1] #Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e40accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The performance metric here will be accuracy defined by rounding the regression score to the nearest whole number,\n",
    "#if that rounded number matches the quality, it is considered correct.\n",
    "def evaluate_regressions(predictions:np.ndarray, y_test:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    predictions is numpy array of regression values predicted by a model\n",
    "    returns the percentage of these rounded values that matched that real wine quality\n",
    "    \"\"\"\n",
    "    predictions = np.round(predictions)\n",
    "    matches = predictions == y_test\n",
    "    return matches.sum()/len(matches) * 100 #Percent correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab55fe34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy for regression: 41.735%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_24760\\1086176822.py:4: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(y_test).mode[0] #Just the mode of the test set\n"
     ]
    }
   ],
   "source": [
    "#First a baseline \"model\" will be created\n",
    "#This will essentially be a model that merely predicts the dataset mode (most frequent value)\n",
    "from scipy import stats #1.10.1\n",
    "mode = stats.mode(y_test).mode[0] #Just the mode of the test set\n",
    "y_base = np.array([mode for sample in X_test])\n",
    "acc_base = evaluate_regressions(y_base, y_test)\n",
    "print(\"Baseline accuracy for regression: {0:.3f}%\".format(acc_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c3f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above percentage is what every model should seek to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffa529a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear model's accuracy for regression: 48.163%\n"
     ]
    }
   ],
   "source": [
    "#Model 1 - linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Training\n",
    "LR_model = LinearRegression()\n",
    "LR_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_LR = LR_model.predict(X_test)\n",
    "acc_LR = evaluate_regressions(y_pred_LR, y_test)\n",
    "print(\"Linear model's accuracy for regression: {0:.3f}%\".format(acc_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd83beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree's accuracy for regression: 57.959%\n"
     ]
    }
   ],
   "source": [
    "#Model 2 - Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Training\n",
    "DTR_model = DecisionTreeRegressor(random_state=0)\n",
    "DTR_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_DTR = DTR_model.predict(X_test)\n",
    "acc_DTR = evaluate_regressions(y_pred_DTR, y_test)\n",
    "print(\"Decision tree's accuracy for regression: {0:.3f}%\".format(acc_DTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab004e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest's accuracy for regression: 63.673%\n"
     ]
    }
   ],
   "source": [
    "#Model 3 - Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Training\n",
    "RFR_model = RandomForestRegressor(random_state=0)\n",
    "RFR_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_RFR = RFR_model.predict(X_test)\n",
    "acc_RFR = evaluate_regressions(y_pred_RFR, y_test)\n",
    "print(\"Random Forest's accuracy for regression: {0:.3f}%\".format(acc_RFR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1bed1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAM's accuracy for regression: 50.816%\n"
     ]
    }
   ],
   "source": [
    "#Model 4 - Generalized Additive Model\n",
    "from pygam import LinearGAM #0.9.0\n",
    "from pygam import s #This is the smoothing function (cubic spline) to be used for continuous features\n",
    "\n",
    "#Training\n",
    "GAM_model = LinearGAM(\n",
    "    s(0) + \n",
    "    s(1) +\n",
    "    s(2) +\n",
    "    s(3) +\n",
    "    s(4) +\n",
    "    s(5) +\n",
    "    s(6) +\n",
    "    s(7) +\n",
    "    s(8) +\n",
    "    s(9),\n",
    "    n_splines=50#Each feature is allowed a max amt of this many splines\n",
    ")\n",
    "GAM_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_GAM = GAM_model.predict(X_test)\n",
    "acc_GAM = evaluate_regressions(y_pred_GAM, y_test)\n",
    "print(\"GAM's accuracy for regression: {0:.3f}%\".format(acc_GAM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5f25135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2 - Algorithm selection for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd903b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct Classification Dataset\n",
    "\n",
    "#Target must now become red or white, not the score\n",
    "df_red = df_red.drop(columns=\"quality\")\n",
    "df_red['color'] = 0 #0 means red\n",
    "df_white = df_white.drop(columns=\"quality\")\n",
    "df_white['color'] = 1 #1 means white\n",
    "\n",
    "all_data = np.vstack((df_red.to_numpy(), df_white.to_numpy()))\n",
    "\n",
    "train, test = train_test_split(all_data, test_size=0.2, random_state=0)\n",
    "X_train = train[:, 0:-1] #Features\n",
    "y_train = train[:, -1] #Target\n",
    "X_test = test[:, 0:-1] #Features\n",
    "y_test = test[:, -1] #Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "001e110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The performance metric here will be accuracy defined by the amount of correct classifications divided by the total.\n",
    "def evaluate_classifications(predictions:np.ndarray, y_test:np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    predictions is numpy array of classification values predicted by a model\n",
    "    returns the percentage of these values that matched that real wine color\n",
    "    \"\"\"\n",
    "    matches = predictions == y_test\n",
    "    return matches.sum()/len(matches) * 100 #Percent correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "424e423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC's accuracy for classification: 93.538%\n"
     ]
    }
   ],
   "source": [
    "#Model 1 - Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Training\n",
    "SVC_model = SVC(random_state=0)\n",
    "SVC_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_SVC = SVC_model.predict(X_test)\n",
    "acc_SVC = evaluate_classifications(y_pred_SVC, y_test)\n",
    "print(\"SVC's accuracy for classification: {0:.3f}%\".format(acc_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3398540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy for classification: 98.692%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\py3_11_2\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Model 2 - Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Training\n",
    "Log_model = LogisticRegression(random_state=0)\n",
    "Log_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_Log = Log_model.predict(X_test)\n",
    "acc_Log = evaluate_classifications(y_pred_Log, y_test)\n",
    "print(\"Logistic regression accuracy for classification: {0:.3f}%\".format(acc_Log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e835daa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy for classification: 98.231%\n"
     ]
    }
   ],
   "source": [
    "#Model 3 - Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Training\n",
    "DTC_model = DecisionTreeClassifier(random_state=0)\n",
    "DTC_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_DTC = DTC_model.predict(X_test)\n",
    "acc_DTC = evaluate_classifications(y_pred_DTC, y_test)\n",
    "print(\"Decision tree accuracy for classification: {0:.3f}%\".format(acc_DTC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45d6bb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy for classification: 94.923%\n"
     ]
    }
   ],
   "source": [
    "#Model 4 - K-nearest neighbor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Training\n",
    "KNN_model = KNeighborsClassifier()\n",
    "KNN_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_KNN = KNN_model.predict(X_test)\n",
    "acc_KNN = evaluate_classifications(y_pred_KNN, y_test)\n",
    "print(\"KNN accuracy for classification: {0:.3f}%\".format(acc_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c367c559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy for classification: 97.615%\n"
     ]
    }
   ],
   "source": [
    "#Model 5 - Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Training\n",
    "GNB_model = GaussianNB()\n",
    "GNB_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_GNB = GNB_model.predict(X_test)\n",
    "acc_GNB = evaluate_classifications(y_pred_GNB, y_test)\n",
    "print(\"Gaussian Naive Bayes accuracy for classification: {0:.3f}%\".format(acc_GNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7abb329b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest's accuracy for classification: 99.538%\n"
     ]
    }
   ],
   "source": [
    "#Model 6 - Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Training\n",
    "RFC_model = RandomForestClassifier(random_state=0)\n",
    "RFC_model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluation\n",
    "y_pred_RFC = RFC_model.predict(X_test)\n",
    "acc_RFC = evaluate_classifications(y_pred_RFC, y_test)\n",
    "print(\"Random Forest's accuracy for classification: {0:.3f}%\".format(acc_RFC))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
